#ifndef _WORKER_H
#define _WORKER_H

// Includes


// Forward declaration
typedef struct __cilkrts_worker __cilkrts_worker;


struct global_state_t { /* COMMON_PORTABLE */

    /* Fields described as "(fixed)" should not be changed after
     * initialization.
     */

    /*************************************************************************
     * Note that debugger integration must reach into the
     * global state!  The debugger integration is depending on the
     * offsets of the addr_size, system_workers, total_workers,
     * stealing_disabled, sysdep, and workers.  If these offsets change, the
     * debugger integration library will need to be changed to match!!!
     *************************************************************************/

    uint16_t addr_size; ///< Number of bytes for an address, used by debugger (fixed)

    uint16_t version;   ///< Version of this structure (fixed)

    int system_workers; ///< Number of system workers (fixed)

    /**
     * @brief USER SETTING: Maximum number of user workers that can be
     * bound to cilk workers.
     *
     * 0 unless set by user.  Call cilkg_calc_max_user_workers to get
     * the value.
     */
    int max_user_workers; 

    int total_workers;  ///< Total number of worker threads allocated (fixed)

    int workers_running; ///< True when system workers have beens started */

    /// Set by debugger to disable stealing (fixed)
    int stealing_disabled;

    /// System-dependent part of the global state
    struct global_sysdep_state *sysdep;

    /// Array of worker structures.
    __cilkrts_worker **workers;

    /******* END OF DEBUGGER-INTEGRATION FIELDS ***************/

    /// Number of frames in each worker's lazy task queue
    __STDNS size_t ltqsize;

    /**
     * @brief USER SETTING: Force all possible reductions.
     *
     * TRUE if running a p-tool that requires reducers to call the reduce()
     * method even if no actual stealing occurs.
     *
     * When set to TRUE, runtime will simulate steals, forcing calls to the 
     * the reduce() methods of reducers.
     *
     */
    int force_reduce;    

    /// USER SETTING: Per-worker fiber pool size
    int fiber_pool_size; 

    /// USER SETTING: Global fiber pool size
    int global_fiber_pool_size;

    /**
     * @brief TRUE when workers should exit scheduling loop so we can
     * shut down the runtime and free the global state.
     *
     * @note @c work_done will be checked *FREQUENTLY* in the scheduling loop
     * by idle workers.  We need to ensure that it's not in a cache line which
     * may be invalidated by other cores.  The surrounding fields are either
     * constant after initialization or not used until shutdown (stats) so we
     * should be OK.
     */
    volatile int work_done;

    int under_ptool;     ///< True when running under a serial PIN tool

    statistics stats;    ///< Statistics on use of runtime

    /**
     * @brief USER SETTING: Maximum number of stacks the runtime will
     * allocate (apart from those created by the OS when worker
     * threads are created).
     *
     * If max_stacks == 0,there is no pre-defined maximum.
     */
    unsigned max_stacks; 

    /// Size of each stack
    size_t stack_size;

    /// Global cache for per-worker memory
    struct __cilkrts_frame_cache frame_malloc;

    /// Global fiber pool
    cilk_fiber_pool fiber_pool;

    /**
     * @brief Track whether the runtime has failed to allocate a
     * stack.
     * 
     * Setting this flag prevents multiple warnings from being
     * issued.
     */
    int failure_to_allocate_stack;

    /**
     * @brief USER SETTING: indicate record or replay log.
     * Set to NULL if not used in this run.
     */
    char *record_replay_file_name;

    /**
     * @brief Record/replay state.
     * Valid states are:
     *   RECORD_REPLAY_NONE - Not recording or replaying a log
     *   RECORD_LOG - Recording a log for replay later
     *   REPLAY_LOG - Replay a log recorded earlier
     */
    enum record_replay_t record_or_replay;

    /**
     * @brief Buffer to force max_steal_failures to appear on a
     * different cache line from the previous member variables.
     *
     * This padding is needed because max_steal_failures is read
     * constantly and other modified values in the global state will
     * cause thrashing.
     */
    char cache_buf[64];

    /**
     * @brief Maximum number of times a thread should fail to steal
     * before checking if Cilk is shutting down.
     */
    unsigned int max_steal_failures;

    /// Pointer to scheduler entry point
    void (*scheduler)(__cilkrts_worker *w);

    /**
     * @brief Buffer to force P and Q to appear on a different cache
     * line from the previous member variables.
     */
    char cache_buf_2[64];

    int P;         ///< USER SETTING: number of system workers + 1 (fixed)
    int Q;         ///< Number of user threads currently bound to workers 
};


#endif
